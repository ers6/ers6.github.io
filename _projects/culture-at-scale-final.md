---
name: Scanning at Scale
tools: [Python, HTML, vega-lite]
image: assets/pngs/cars.png
description: This is a "showcase" project that uses vega-lite for interactive viz!
custom_js:
  - vega.min
  - vega-lite.min
  - vega-embed.min
  - justcharts
---

# Scanning at Scale
> ​​“ We’ve chosen scale, and the conceptual apparatus to manage it, at the expense of finer-grained knowledge that could make a more just and equitable arrangement possible.” (Posner)

In a former Christian Science church in San Francisco, petabytes of servers store thousands of cassettes, millions of books, and billions of web pages. This place where technophiles come to worship is known as the Internet Archive. Founded in 1996 by a dot com era billionaire, Brewster Kahle, the Internet Archive initially aimed to archive all of the world wide web. 27 years later in 2023, the Internet Archive is now the world’s largest free digital library boasting over 800 billion webpages, 37 million books and texts, 15 million audio recordings, 9.7 million videos, 4.6 million images, and 983 thousand software programs. As such, it is a crucial piece of scholarly infrastructure enabling the study of culture at scale.

Aside from the 800 billion webpages, the digitization of which can be automated via web crawlers, human workers digitize all other content of the internet archive. This project is concerned with the digitization process through which the second most common content type--books--were ingested into the archive. Internet Archive announced the launch of its book scanning program with a December 2004 blog post. According to the post, the program began as a partnership between Internet Archive and ten academic libraries across the world in an effort to digitize free, open access versions of books made available to the public via the web (Kaplan “Open Text Archives”). In the months that followed, Internet Archive set up scanning centers in partner-institution libraries. While Internet Archive celebrates some scanning workers and scanning centers on their blog, the process through which the books are scanned remains opaque. 

*[Scanning Labor in the Internet Archive](https://scanninglabor.github.io/IAScanningLabor/index.html)* is a DH project that seeks to remedy this gap. Through a data-enabled spatial, labor, and oral history, Scanning Labor sheds light on and reappraises the work of book scanning workers in the Internet Archive. Scanning Labor analyzed 2.5 million metadata records of books and texts that IA scanning workers digitized between 2004 and 2022. Each scanning record contains information about the date upon which the book was scanned and the name of the scanning center at which the scan was captured. While the 2.5 million records are a small subset of all the books and texts in the internet archive, analysis of them revealed that around 2011 Internet Archive shifted most of its book scanning operations from in-house scanning centers at academic libraries in the global north to shipping vast quantities of books overseas for digitization at business process outsourcing firms in the global South. The shift from academic library scanning operations is also associated with a sharp uptake in the number of total books scanned per month and the number of books scanned per worker per day. However, IA rarely mentions these outsourced scanning centers in public press releases, and when pressed about them, Internet Archive officials ceased communication with our team and engagement with the *Scanning Labor* project.

<vegachart schema-url="{{ site.baseurl }}/assets/json/total_book_scans.json" style="width: 100%"></vegachart> 

This current project investigates the outsourcing pattern that the *Scanning Labor* team identified through pairing analysis of scanning metadata records with bills of lading that record the import of goods from overseas suppliers to Internet Archive’s US locations. This project is interested in piecing these two data sources together to understand the infrastructure and workers who have made possible internet archive’s production of culture at scale through visualizing probable connections between suppliers, scanning centers, scanning workers, and the books they digitized. Ultimately, this project is an experiment in computationally-aided critical digital humanities. Through my never certain speculative data visualizations, I attempt to use computational techniques to re-embed IA data in the contexts of its creation, reappraise the work of the people who made it, and through doing so breakdown the black boxing of data, workers, and users that I will suggest culture at scale hinges upon.

## A Note On Scale

From IA’s book scanning project’s start in 2004, scanning workers have added 37 million books to the website. And yet, every book contained within it--be it scanned in 2008 or 2023, is rendered in the same way. A flattened image of a book page on a black background. The platform for navigating them is uniform, the location of the metadata for them, always below, and even the dimensions of each book appear identical. Indeed, for these books to be made discoverable and readable on the platform, they must be made to scale.

<img src="/assets/pngs/ia-platform-uiuc-scanned-book.png" alt="">

Scale, according to Anna Tsing, describes a system that can expand without re-thinking the elements arranged within it. This particular type of expansion “is possible only if project elements do not form transformative relationships that might change the project as elements are added”  (Tsing 507). As such, a scalable system by definition precludes diversity of the objects and agents included within it and bars relationships among its elements. For example, a standard query language database is a scalable system in the sense that each object in the database has predetermined relationships with other elements within it, but any new relationships that an added record may bring with it cannot be rendered into the database without changing its underlying design. Naturally occurring systems, of course, do not scale. For there are no real relationships that do not transform constituent parts, and expansion without change is not actually possible. Rather, these so called scalable projects are “at best…articulations between scalable and nonscalable elements, in which nonscalable effects can be hidden from project investors” (Tsing 515). 

Scalable projects hide transformative relationships through their modular design. Miriam Posner suggests that “modular systems manage complexity by “black-boxing” any information or relationships that do not need to be known at a node in a system for the whole system to function.” (Posner).  Posner offers the example of the shipping container which manages the complexity of global supply chains through making it such that “one doesn’t need to know what’s in the box, just where it needs to go” (Posner).  In the case of a SQL database, the design makes invisible connections between records in the database that do not share query-able keys, or elements that are the same across all elements of the database. 

Leveraging a modular database design, Internet Archive scans books at scale through obfuscating transformative relationships between the digitized book scans, physical codices, scanning centers, scribe machines, and scanning workers that created them. Internet archive’s scanning infrastructure consists of 4 interlocking parts: books, scanning centers, workers, and scribe machines. Through replicating this arrangement of material resources, tools, and labor, internet archive has been able to scale up their scanning program to digitize 37 million books. In the case of IA, the database infrastructure hides these transformative relationships from a user through representing books identically regardless of scanning center and making metadata fields like scanning centers and scanning operators searchable only via custom query. However, through mining book metadata, this project has made visible the particularities of each scanning center and has begun to unearth the transformative relationships between book, scanning center, machine, and human operator.

## DH and (Mass) Digitization

Text-based mass digitization projects like that of Internet Archive differ from their earlier counterparts due to their industrial scale, totalizing aims, and speed. Prior to the development of optical character recognition technologies, humans transcribed text on book pages by hand to create machine readable renderings of printed content (Coyle 642). Mass digitization projects, on the other hand, involve the photographing of each book page and running OCR programs on these images to produce searchable plain text versions. Early mass digitization projects, such as JSTOR in 1995 and the Million Books Project in 2001, were launched at academic libraries with funding from the Mellon Foundation and National Science Foundation respectively (Guthrie; Reddy and St. Clair). Silicon Valley got involved soon after. The Google books project, code named Project Ocean, began covertly 2002 as a partnership between Google and the University of Michigan when google developers offered to digitize U-M library’s entire collection free of charge under one condition: the library had to sign a nondisclosure agreement. The company did not announce the project publicly until 2004. 7 million books later, Google’s scanning processes, locations of scanning centers, and the names of the people who worked there remain unknown. After the Google Book’s project went public, Internet Archive unveiled a book scanning program of its own (Kaplan “Open Text Archive”). Next, in 2005, Microsoft began scanning books for a project called MSN Book search (later Live Search Books) with its platform going live in December 2006 (Quint). As part of its efforts, Microsoft partnered with IA, supplying the archive with money and equipment for book scanning activities. Through 2007 the number of Google, Internet Archive, and Microsoft scanning programs expanded rapidly before the landscape shifted dramatically in 2008.

Prior to 2008 the major use case for mass digitization-as envisioned by Google and Microsoft-was to provide better answers to search engine queries. However, in 2008, with the shuttering of Microsoft’s project and creation of HathiTrust, digital libraries became predominant. In May 2008, Microsoft abruptly shut down its scanning projects, and Internet Archive inherited the program’s public domain scans, their scanning equipment, and many of their partnerships (Schiffman; Kaplan “Book Scanning”). This left Internet Archive and Google as the only two major organizations involved in book mass digitization projects. Then in October of that same year, the HathiTrust digital library launched containing in it all the digitized books from the Google Books partnered libraries (“Launch of HathiTrust”). Unlike the Google Books and Microsoft Live Search platforms, Internet Archive and HathiTrust make possible the reading of digitized books via a browser and allow the download of plain text versions of them. This increased public and academic access to digitized texts in quantities earlier digitization projects could only dream of.   

Prior to the inauguration of mass digitization projects in the early 2000s, much of digital humanities scholarship focused on how to remediate analog documents to enable their computational study. The earliest digital humanities projects (or at least those carried out on an electric computer) took place in the immediate post-War era were computationally aided philological analysis. For example, Father Antonio Busa’s Index Thomisticus, conducted in partnership with IBM, aimed to identify textual patterns in acquina’s work. To do so, Busa worked in tandem with IBM engineers to develop a method for rendering his index in machine readable punch cards for large scale analysis. With the creation of affordable desktop computers by the 1980s, the subsequent establishment of TEI, and the public launch of the world wide web in the 1990s, digital archiving projects boomed (Hockey). In 1991, the University of Virginia began the [Valley of the Shadow](https://valley.lib.virginia.edu/) project in history and then, in 1993, the [William Blake Archive](https://www.blakearchive.org/) in literary studies. For both projects, project teams undertook the collection, digitization, transcription, and encoding of the materials themselves. The Valley team, for example, “began the seemingly endless task of collecting, transcribing, and converting original source material into computer readable files with a few hours of work-study graduate students” after which graduate student, Anne Rubin, lead the conversion of the transcribed plain text into standardized general markup language (“The Story Behind”). In both the project sites for the Valley and Blake projects digitization labor is described not just as arduous but also as worthwhile scholarly labor. Across all three of these early DH projects, digitization of analog documents was considered scholarship. 

Thanks in part to mass digitization of book data, a growing number of studies started to leverage computational methods to study literary history at scales that were previously unimaginable. Computer-aided text analysis is a practice dating back to 1945 at least, while the reading of text at macroscopic levels developed independently of computational humanities (Underwood). However, as Ted Underwood suggests, these two fields of macroscopic literary analysis and humanities computing only began to merge around the 2000s with Franco Moretti’s publication of “Conjectures on World Literature” (Underwood). Computationally driven literary history studies at scale picked up by the 2010s with the publications of *Graphs, Maps, and Trees* (2005), *An Algorithmic Criticism* (2011), and *Macroanalysis* (2013). All three of these monographs, but especially that of Moretti are marked by a “view of literary data as factual and transparent” which Katherine Bode attributes to “Moretti’s lack of interest in the scholarly infrastructure that enables his analyses” (Bode 21). Here, Bode is referring to a lack of attention to the processes of collection that determines what books are present in academic libraries in the first place. But also notably absent in all of these texts is any acknowledgement of digitization labor of any kind. Indeed while DH projects of the 1990s use digitization as a primary method (a kind of scholarly labor to be praised), in these distant reading projects, digitization is not scholarly labor or labor of any kind. It is not mentioned at all. 

The rise of distant reading that proliferated due to corporate-backed, outsourced mass digitization coincided with the invisibilization of digitization labor and subsequent treatment of digitized texts as surrogates in hegemonic digital scholarship. This shift had to do less with a neglect of scholarly infrastructure, as previous scholars have theorized, and more to do with the modular design of scalable systems that mass digitization projects demanded. For TEI-driven projects, digitization labor oftentimes took place in academic libraries by library staff who were often deeply involved in the projects themselves. In the case of the [Edmund Spenser Digital Edition](https://talus.artsci.wustl.edu/spenserArchivePrototype/) project (which began in 1998), scholars worked closely with archivists across the world to identify all extant copies of first-editions of Spenser’s works. By the time scanning work began, the principle investigators had already built relationships with people at the institutions and in the libraries that housed the documents to be digitized. In contrast, for his 2019 *Distant Horizons*, Ted Underwood is analyzing texts from the HathiTrust corpus. The codices digitized in the HathiTrust project came from a number of libraries across the US. Almost all were digitized via the Google books program, which agreed to digitize academic libraries’ collections in outsourced, undisclosed locations under non-disclosure agreements. Underwood, unlike the Spenser team, worked with material already digitized in processes that HathiTrust’s metadata fields and Google’s NDAs make unknowable. It’s not just the distant readers like Underwood neglect scholarly infrastructure; it’s that the production of culture at scale obscures infrastructure entirely.

Data archaeology is a method that aims to excavate the infrastructure that digitizations and databases obscure. In her 2014 article on the history of EEBO, Bonnie Mak introduces archaeology as a methodology for elucidating “the discursive practices by which digitizations are produced, circulated, and received” (Mak 1515). While Mak acknowledges—and, in moments, details--that the digitizations in EEBO’s corpus are the product of historical infrastructure and human labor, she is primarily concerned with how these arrangements imbue the digitized text with meanings that ultimately reshape our understanding of the past. Since her 2014 article, other scholars have used archaeology to study the infrastructure, discourses, and politics that produce digital objects (Cordell “Qtjb”; Fyfe; Lee). But like Mak, these scholars are ultimately more concerned with data/objects—not the people most materially impacted by their creation. 

More recently, digital humanist, Safiya Noble, has insisted on thinking critically about how DH infrastructure is complicit in systems of global racial capitalism. In her contribution to the 2019 *Debates in DH*, “Towards a Critical Black DH,” Noble observes that digital humanists have become increasingly interested in digitizing cultural productions of communities that have been excluded from the field without using digital tools to dismantle the systems that perpetuate these communities’ systematic exclusion. And yet, Noble notes, that

> We can no longer deny that digital tools and projects are implicated in the rise in global inequality, because digital systems are reliant on global racialized labor exploitation. We can no longer pretend that digital infrastructures are not linked to crises like global warming and impending ecological disasters. We cannot deny that the silences of the field in addressing systemic state violence against Black lives are palpable. Critical digital humanities must closely align with the register in which critical interventions can occur. (Noble)

The critical DH that Noble calls for  “foreground[s] a recognition of the superstructures that overdetermine the computerization and informationalization of DH projects, so that those projects can intervene in instances of racial, economic, and political oppression.” Put differently, critical DH goes beyond recognizing its complicity; it seeks to use our methods to dismantle and reimagine the exclusionary systems that make our work possible in the first place. Since Noble’s 2019 intervention, some digital humanists have turned their attention to the extractive infrastructure that enables DH research and tools (Klein 2022), but we have yet to explore how to use digital tools to undermine and even topple extractive DH infrastructure. Scanning at Scale leverages computational data analysis and visualization to read IA book scanning records against Internet Archive’s bills of lading in order to locate the transformative relationships between the nonscalable elements of IA’s scanning infrastructure that make the scanning at scale possible but are necessarily concealed.


# Map 

<vegachart schema-url="{{ site.baseurl }}/assets/json/connections-map.json" style="width: 100%"></vegachart>

# All scans att the centers

<vegachart schema-url="{{ site.baseurl }}/assets/json/outsourced_centers.json" style="width: 100%"></vegachart>


# Datum Data Co. Ltd. 

<vegachart schema-url="{{ site.baseurl }}/assets/json/datum_data_ship_viz.json" style="width: 100%"></vegachart>

# Internet Archive China

<vegachart schema-url="{{ site.baseurl }}/assets/json/hongkong_ships_viz.json" style="width: 100%"></vegachart>

# Innodata Knowledge Services

<vegachart schema-url="{{ site.baseurl }}/assets/json/cebu_ship_viz.json" style="width: 100%"></vegachart>


<!-- these are written in a combo of html and liquid --> 


{% include elements/button.html link="https://github.com/vega/vega/blob/main/docs/data/cars.json" text="The Data" %}
</div>

<div class="right">
{% include elements/button.html link="https://github.com/jnaiman/online_cv_public/blob/main/python_notebooks/test_generate_plots.ipynb" text="The Analysis" %}
</div>
